From: https://www.linkedin.com/pulse/reinforcement-learning-from-human-feedback-rlhf-train-shankar-kanap/

**Reinforcement Learning from Human Feedback (RLHF)** is a powerful technique that allows us to train machine learning models by providing human feedback. This approach is particularly useful for training Large Language Models (LLMs), which can be challenging to train using traditional supervised learning methods.

To illustrate how RLHF can be used to train an LLM, let's consider the task of generating product descriptions. In this example, the LLM must generate a short description of a product that accurately conveys its key features and benefits.

![Base LLM vs Instruction Tuned LLM](images/rlhf.png)    


